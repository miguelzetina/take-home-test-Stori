# take-home-test-Stori
Technical Challenge for Stori Software Enginner

# Serverless Reference Architecture: Real-time File Processing

The Real-time File Processing reference architecture is a general-purpose, event-driven, parallel data processing architecture that uses [AWS Lambda](https://aws.amazon.com/lambda). This architecture is ideal for workloads that need more than one data derivative of an object.

In this application, we deliver email with summary account data in a CSV file from S3 Bucket. S3 Events are used to trigger a processing flow - this convert, persist, calculate and send summary data from CSV file to an email.

## Application Components

### Event Trigger

In this architecture, individual files are processed as they arrive. To achive this, we utilize AWS S3 Events. When an object is created in S3, an event is emitted to a created function Lambda.

### Process Transactions File Workflow

Our function will take CSV files stored in our **InputBucket**, read content, save in **TxnTable** and calculate summary and convert to HTML.

## Building and Deploying the Application with the AWS Serverless Application Model (AWS SAM)

This application is deployed using the [AWS Serverless Application Model (AWS SAM)](https://aws.amazon.com/serverless/sam/). AWS SAM is an open-source framework that enables you to build serverless applications on AWS. It provides you with a template specification to define your serverless application, and a command line interface (CLI) tool.

### Pre-requisites

* [AWS CLI version 2](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)

* [AWS SAM CLI (0.41.0 or higher)](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html)

* [Docker](https://docs.docker.com/install/)

### Clone the Repository

#### Clone with SSH

```bash
git clone git@github.com:miguelzetina/take-home-test-Stori.git
```

#### Clone with HTTPS

```bash
git clone https://github.com/miguelzetina/take-home-test-Stori.git
```

### Build

The AWS SAM CLI comes with abstractions for a number of Lambda runtimes to build your dependencies, and copies the source code into staging folders so that everything is ready to be packaged and deployed. The *sam build* command builds any dependencies that your application has, and copies your application source code to folders under *.aws-sam/build* to be zipped and uploaded to Lambda. 

```bash
sam build --use-container
```

**Note**

Be sure to use v0.41.0 of the AWS SAM CLI or newer.  Failure to use the proper version of the AWS SAM CLI will result in a `InvalidDocumentException` exception.  The `EventInvokeConfig` property is not recognized in earlier versions of the AWS SAM CLI.  To confirm your version of AWS SAM, run the command `sam --version`.

### Deploy

For the first deployment, please run the following command and save the generated configuration file *samconfig.toml*. Please use **take-home-test-stori** for the stack name.

```bash
sam deploy --guided
```

Subsequent deployments can use the simplified `sam deploy`.  The command will use the generated configuration file *samconfig.toml*.

## Testing the Example

After you have created the stack using the CloudFormation template, you can manually test the system by uploading a CSV file to the InputBucket that was created in the stack.

### Manually testing

You can use the file stori_account_1234567890.csv file in the repository /**app**/**examples** directory. After the files have been uploaded, you can see the transactions saved in **TxnTable** table of your stack. You can also view the CloudWatch logs for each of the functions in order to see the details of their execution.

You can use the following commands to copy a sample file from the provided S3 bucket into the input bucket of your stack.

```bash
INPUT_BUCKET=$(aws cloudformation describe-stack-resource --stack-name take-home-test-stori --logical-resource-id InputBucket --query "StackResourceDetail.PhysicalResourceId" --output text)
aws s3 cp ./app/examples/stori_account_1234567890.csv s3://${INPUT_BUCKET}/stori_account_1234567890.csv
```

Once the input files has been uploaded to the input bucket, a series of events are put into motion.

1. The input CSV file are readed, converted and stored in a DynamoDB table.
```
DYNAMO_TABLE=$(aws cloudformation describe-stack-resource --stack-name take-home-test-stori --logical-resource-id TxnTable --query "StackResourceDetail.PhysicalResourceId" --output text)
aws dynamodb scan --table-name ${DYNAMO_TABLE} --query "Items[*]"
```

You can also view the CloudWatch logs generated by the Lambda functions.

## Reference to this architecture
* [Serverless Reference Architecture: Real-time File Processing](https://github.com/aws-samples/lambda-refarch-fileprocessing)
